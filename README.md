# PySpark Linker Продуктов и Категорий

Этот проект представляет собой простое приложение PySpark, которое демонстрирует, как связывать продукты с их соответствующими категориями. Он принимает DataFrame'ы для продуктов, категорий и их связей, и выводит DataFrame, показывающий каждый продукт и его категорию. Продукты без назначенной категории будут отображать `null` в качестве имени категории.

## Основная Логика

Основная логика находится в `src/logic.py`, в частности, в функции `products_with_categories`. Эта функция выполняет следующие шаги:
1. Объединяет таблицу сопоставления `product_category` с таблицей `categories` для добавления названий категорий.
2. Объединяет таблицу `products` с результатом из шага 1, используя левое соединение, чтобы гарантировать включение всех продуктов, даже если у них нет категории.
3. Выбирает и возвращает `product_name` и `category_name`.

Пример использования этой функции с образцами данных представлен в `main.py`.

## Как Запустить Пример

Чтобы запустить пример в `main.py`, который инициализирует сессию Spark и обрабатывает образцы данных:

1.  Убедитесь, что у вас установлен и настроен PySpark.
2.  Перейдите в корневую директорию проекта.
3.  Выполните следующую команду:
    ```bash
    python main.py
    ```

## Структура Проекта

- `main.py`: Точка входа для запуска демонстрационного примера связывания продуктов и категорий.
- `src/`: Содержит основную логику приложения.
  - `src/logic.py`: Модуль с функциями преобразования данных (например, `products_with_categories`).
- `tests/`: Содержит модульные тесты.
  - `tests/test_logic.py`: Модульные тесты для функций из `src/logic.py`.
- `README.md`: Этот файл.
- `requirements.txt`: Перечисляет зависимости проекта (в основном PySpark).
- `.gitignore`: Указывает намеренно неотслеживаемые файлы, которые Git должен игнорировать.

## Дальнейшие Улучшения и Вклад

Этот проект является базовым примером. Потенциальные области для улучшения или вклада включают:

*   Добавление более сложных сценариев преобразования данных.
*   Реализация более комплексной обработки ошибок.
*   Расширение набора тестов для покрытия большего количества крайних случаев.
*   Оптимизация заданий Spark для повышения производительности на больших наборах данных.
*   Добавление опций конфигурации (например, для параметров сессии Spark, путей ввода/вывода).

### Вклад
Если у вас есть идеи по улучшению или вы обнаружили проблемы:
1.  Сделайте форк репозитория.
2.  Создайте новую ветку для ваших изменений.
3.  Внесите изменения, убедившись, что добавили или обновили тесты, если это применимо.
4.  Создайте Pull Request для рассмотрения.

Мы приветствуем любой вклад!
